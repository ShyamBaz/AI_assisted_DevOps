# local-llms-ollama-setup

This folder contains instructions and resources for setting up local Large Language Models (LLMs) using Ollama.

## Overview
- Ollama enables running open-source LLMs locally for development, experimentation, and integration.

## Getting Started
1. Install Ollama on your system (see https://ollama.com/download).
2. Follow setup instructions for your preferred LLM model.
3. Add scripts, configuration files, and notes in this folder as needed.

## Example Usage
- Add example commands, setup steps, or troubleshooting tips here as you build your local LLM environment.

---

For more information, see the parent GenAI_Project README.
